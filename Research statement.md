# Research statement

Dear Professors,

It is my pleasure to introduce my research and work experience to you. 

I am currently pursuing my master's degree at the University of Science and Technology of China, and my current main research interests are machine learning and robot human-robot interaction.

In my past studies, I have participated in the lab's Kejia robot project, Jiajia humanoid robot project and giant panda robot project, which involve topics such as robot control, computer vision and natural language processing.

Below I will describe my role in these projects and the specific tasks I was responsible for.

Jiajia humanoid robot is a robot with human appearance, facial expressions and movements, and natural language dialogue capabilities developed in our lab. In 2018, the State Grid Group of China and our lab started a collaborative project for the development of robotic customer service, in which there was a requirement that after a customer interacts with a robot, the robot should be able to make an analysis of the customer's feedback and determine the emotional polarity and the source of emotion generated by the customer's feedback. After being assigned this task, my colleagues and I first conducted data collection in real sites, collecting a large amount of data in the form of audio recordings, comment books, and web message boards, and then obtained a preliminary dataset after data cleaning, labeling, and augmentation. Firstly, we tested statistical learning models such as SVM, XGBOOST, LightGBM and tree models using TF-IDF as the feature extraction function. We then measured the attention-based pre-trained language models, BERT and DistilBERT, etc. The output layer is responsible for outputting the category probabilities for both classification tasks. The final best accuracy of 93.7% was obtained with the BERT model on the dataset, and eventually this model was actually deployed on the robot platform.

The giant panda simulation robot is a derivative project of Kejia Robotics, which was conducted by the lab in collaboration with the Chengdu Giant Panda breeding Base in Sichuan. The robot has realistic giant panda fur, movable arms, a face that can perform expressive movements, and an autonomously movable chassis system from the Kejia project. In this project, I was assigned to work on the development of the robot's active interaction and autonomous movement functions.

After being given this task, my classmates and I researched and analyzed the project. The robot activity site is located in the hall of a museum, the maximum hourly flow of people is about 2,000 people, when the crowd is dense, the robot's activities should be fully considered pedestrian factors. The first is to ensure the safety of the robot when cruising. Through the fusion of LIDAR and ultrasonic radar sensing data and SLAM navigation, we obtained the preliminary map and positioning information. We use the depth camera and YOLOv5 model to detect pedestrian targets and obtain the spatial coordinates of pedestrians in the field, for which we can avoid pedestrians or actively approach specific pedestrians during the movement.

Second, the active interaction function is also a key point, when there are fewer people in the venue or the robot is snubbed, it has to rely on the sensor judgment to actively approach the target and initiate interactions. First, we use the HydraPlus vision model to obtain the age, gender, and other attributes of the pedestrians, and the robot uses these attributes to filter the people most likely to respond to the interaction request and actively approach them to initiate the interaction. The active interaction is initiated by a combination of voice prompts and the GUI front-end interface, which guides the user to interact with the robot through the featured content.

Finally, the robot operates in a complex environment, so it needs safety and robustness assurance. We use the sensor system to design a QR code based positioning recovery function, a tipping detection and a climbing detection system, which can alert the staff in case of danger and allow them to resume robot back to its normal operation.

Out of lab projects I have participated in internships at several companies regarding machine learning and algorithms. In March 2021, I worked as an intern on the "talking face with upper body" project at Microsoft Research Asia and the Chinese University of Hong Kong, which is dedicated to synthesizing smooth facial videos with upper body movements from text or speech. During this work, I did a comprehensive survey of  past works on mainstream talking face models, and conducted preliminary data collection and collation using crawlers on YouTube and other video websites.

In June 2021, I participated in Ant Group as an intern machine learning engineer. During this work, I was involved in the development of Ray, a distributed framework, and pyMars, a distributed machine learning platform, where I was responsible for the research of existing distributed LightGBM models and the development of LightGBM for distributed training on the pyMars platform. and the development of the failover mechanism in Ray.

In addition to these tasks, I have participated in many machine learning and data science competitions on AliCloud platform or kaggle platform. One of the more impressive ones is tax data risk identification, where we use the community discovery algorithm of graph to identify community information on the interpersonal graph. Then we can discover fraudulent groups and their connections in tax registration information. Through multi-angle feature extraction and convolutional neural network in the temporal dimension, we can finally determine whether there is tax fraud in a corporate’s tax data. This work has been actually deployed in Wuhu City's smart city platform.

In addition is the El Niño indicator prediction competition of AliCloud, where the organizer provides global meteorological data for the past two years and asks us to predict the El Niño indicator data in the coming year. We first extracted features from the weather maps at each time point through a residual convolutional neural network, and then predicted El Niño indicators for the next twelve months through an attention-based RNN or Transformer by a Seq2Seq method.

The above is my research and work experience. During the past few years of research, I have been involved in several areas of machine learning, so I have the interest and confidence to shift my research direction and participate in this PhD project, and I think my advantages are.

1. I have a good theoretical foundation in machine learning and rich experience in engineering development, I have participated in many research and engineering projects involving machine learning, I can analyze problems and model design and construction quickly, and manage hardware clusters and software architectures

2. I have a rich research and engineering development experience in computer vision and NLP, and I have been involved in dialogue system and object detection during my internship and research works.

3. I am proficient in using Pytorch, I trained myself in many machine learning competitions, and I am also proficient in using TensorFlow and Keras.

Here are two things that I think make me a better match for this position,

4. I have a good skill in information collection, I can use tools like crawlers, Google scholar, Zotero and overleaf LaTex to do literature survey, management and paper writing

5. I have experience in open source developments and I can use Git, Docker to manage versions of project code and development envoirments.

These are my personal statements, and I hope to have the opportunity to work with you in the future to make artificial intelligence benefit society and more individuals!

